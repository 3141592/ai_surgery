# Introduction

```
01-15-2026
```
## What question pulled me here?

- What is TransformerLens? What does it do?

## What did I actually do?

- Ran some copy/paste scripts from ChatGPT.
- Realized I want to know more about transformers, attention and head before proceeding.
- Got a book recommendation from my pair programmer for this: Dive into DL

- Attention chapter
  - scaled dot-product attention
  - why Q/K/V exist
  - Multi-Head Attention section
  - what differs between heads
  - why outputs are concatenated
- Transformer chapter
  - one encoder block
  - attention → MLP → residuals

## What do I think I understand a little better now (or still don’t)?

My working mental model is that the data does exist inside transformer models to be able to
probe them to understand better how they work.

```
01-25-2026
```
Adding support for pytorch and keras, and an example of Transformer encoding created by ChatGPT.



