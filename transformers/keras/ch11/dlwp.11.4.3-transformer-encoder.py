import os, pathlib
from pathlib import Path
import sys

os.environ["TF_XLA_FLAGS"] = "--tf_xla_auto_jit=0"
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
from ai_surgery.data_paths import get_data_root

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

DATA_ROOT = get_data_root() / "aclImdb"

MODEL_PATH = (
    get_data_root()
    / "models"
    / "transformer_encoder.keras"
)

print("11.3.4 Using pretrained word embeddings")
import tensorflow as tf
from tensorflow import keras

# IMPORTANT NOTE ABOUT DATA SPLITTING:
#
# Earlier versions of this repo used a physical `val/` directory copied
# from the training set. That approach can silently introduce data leakage
# if files are duplicated between train and validation.
#
# In smaller models this leakage may not visibly distort results.
# However, higher-capacity models (e.g., the Transformer encoder)
# can memorize duplicated samples, producing artificially high
# validation accuracy.
#
# To prevent this, we now generate validation data directly from the
# official `aclImdb/train` directory using `validation_split`
# with a fixed seed. This ensures a clean, reproducible split.

batch_size = 16
seed = 1337
val_split = 0.2  # 20% of train -> val

train_ds = keras.utils.text_dataset_from_directory(
    DATA_ROOT / "train",
    batch_size=batch_size,
    validation_split=val_split,
    subset="training",
    seed=seed,
)

val_ds = keras.utils.text_dataset_from_directory(
    DATA_ROOT / "train",
    batch_size=batch_size,
    validation_split=val_split,
    subset="validation",
    seed=seed,
)

test_ds = keras.utils.text_dataset_from_directory(
    DATA_ROOT / "test",
    batch_size=batch_size,
    shuffle=False,
)

text_only_train_ds = train_ds.map(lambda x, y: x)

print("Train:", train_ds.cardinality())
print("Val:", val_ds.cardinality())
print("Test:", test_ds.cardinality())

print("Listing 11.12 Preparing integer sequence datasets")
from tensorflow.keras import layers

max_length = 600
max_tokens = 20000
text_vectorization = layers.TextVectorization(
        max_tokens=max_tokens,
        output_mode="int",
        # In order to keep a manageable input size, we'll truncate the inputs after the first 600 words.
        output_sequence_length=max_length,
)
text_vectorization.adapt(text_only_train_ds)

int_train_ds = train_ds.map(
                lambda x, y: (text_vectorization(x), y),
                num_parallel_calls=tf.data.AUTOTUNE)
int_val_ds = val_ds.map(
                lambda x, y: (text_vectorization(x), y),
                num_parallel_calls=tf.data.AUTOTUNE)
int_test_ds = test_ds.map(
                lambda x, y: (text_vectorization(x), y),
                num_parallel_calls=tf.data.AUTOTUNE)



print("Listing 11.21 Transfgormer encoder implemented as a subclassed layer")
class TransformerEncoder(layers.Layer):
    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
        super().__init__(**kwargs)
        # Size of the input token vectors
        self.embed_dim = embed_dim
        # Size of the inner dense layer
        self.dense_dim = dense_dim
        # Number of attention heads
        self.num_heads = num_heads
        self.attention = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim)
        self.denseproj = keras.Sequential(
            [layers.Dense(dense_dim, activation="relu"), 
             layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization()
        self.layernorm2 = layers.LayerNormalization()

    # Computation goes in call().
    def call(self, inputs, mask=None):
        # The mask that will be generated by the Embedding layer will be 2D,
        # but the attention layer expects to be 3D or 4D, so we expand its rank.
        if mask is not None:
            mask = mask[:, tf.newaxis, :]
        attention_output = self.attention(
            inputs, inputs, attention_mask=mask)
        proj_input = self.layernorm1(inputs + attention_output)
        proj_output = self.denseproj(proj_input)
        return self.layernorm2(proj_input + proj_output)
    
    # Implemnent serialization so we can save this model.
    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim": self.embed_dim,
            "dense_dim": self.dense_dim,
            "num_heads": self.num_heads,
        })
        return config    
    
    def build(self, input_shape):
        # Let Keras know this layer is built.
        # Sublayers (attention, denseproj, layernorm) build themselves
        # automatically the first time they are called.
        super().build(input_shape)

print("Listing 11.22 Using the Transformer encoder for text classification")
vocab_size = 20000
embed_dim = 256
num_heads = 2
dense_dim = 32

inputs = layers.Input(shape=(None,), dtype="int64")
x = layers.Embedding(vocab_size, embed_dim)(inputs)
x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)
x = layers.GlobalMaxPooling1D()(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop",
        loss="binary_crossentropy",
        metrics=["accuracy"])
model.summary()

print("Listing 11.23 Training and evaluating the Transformer encoder based model")
callbacks = [
    keras.callbacks.ModelCheckpoint(MODEL_PATH,
                                    save_best_only=True),
    EarlyStopping(monitor="val_loss",
                  patience=3,
                  restore_best_weights=True)
]
print("Training the model...")
model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)

print("Evaluating the model on the test set...")    
# Provide the custom TransformerEncoder class to the model-loading process.
model = keras.models.load_model(MODEL_PATH, 
                                custom_objects={"TransformerEncoder": TransformerEncoder})
print(f"Test accuracy: %.3f" % model.evaluate(int_test_ds)[1])
